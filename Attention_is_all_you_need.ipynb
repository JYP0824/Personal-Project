{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAZtBhQWyscPODP6gFLdw8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JYP0824/Personal-Project/blob/main/Attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "8ksKGyqlZ9N2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EcuKA6GL3N-C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PE(nn.Module):\n",
        "  def __init__(self, seq_len, d_model):\n",
        "    super(PE, self).__init__()\n",
        "\n",
        "    self.encoding = torch.zeros(seq_len, d_model, device=device)\n",
        "    encoding.requires_grad=False\n",
        "\n",
        "    pos = torch.arange(0,seq_len, device=device)\n",
        "    pos = pos.float().unsqueeze(dim=1)\n",
        "\n",
        "    _2i = torch.arange(0, d_model, step=2, dvice=device)\n",
        "\n",
        "    encoding[:,0::2] = torch.cos(pos/10000**(_2i/d_model))\n",
        "    encoding[:,1::2] = torch.sin(pos/10000**(_2i/d_model))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len  = x.size()\n",
        "    return encoding[:seq_len, :]"
      ],
      "metadata": {
        "id": "tYow1Jxd3dHc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotproductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ScaledDotproductAttention, self).__init__()\n",
        "\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "    batch_size, num_head, length, d_k = k.size()\n",
        "    k_t = torch.view(batch_size, num_head, d_k, length)\n",
        "    score = (q @ k_t) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score.masked_fill(mask ==0, -e)\n",
        "\n",
        "    score = self.softmax(score)\n",
        "    v = score @ v\n",
        "\n",
        "    return v, score"
      ],
      "metadata": {
        "id": "q6bqj8Ek3dK7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_head):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_concat = nn.Linear(d_model, d_model)\n",
        "    self.attention = ScaledDotproductAttention()\n",
        "\n",
        "  def split(self, tensor):\n",
        "    batch_size, head, length, d_model = tensor.size()\n",
        "\n",
        "    d_k = d_model // self.n_head\n",
        "    tensor = tensor.view(batch_size, self.n_head, length, d_k)\n",
        "    return tensor\n",
        "\n",
        "  def concat(self, tensor):\n",
        "    batch_size, head, length, d_k = tensor.size()\n",
        "    d_model = head * d_k\n",
        "\n",
        "    tensor = tensor.view(batch_size, length, d_model)\n",
        "    return tensor\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    q,k,v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "    q,k,v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "    temp1, attention = self.attention(q,k,v,mask=mask)\n",
        "    temp2 = self.concat(temp1)\n",
        "    output = self.w_concat(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "yd8AS6gV3dSu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self,d_model,eps = 1e-12):\n",
        "        super(LayerNorm,self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self,x):\n",
        "        mean = x.mean(-1,keepdim = True)\n",
        "        std = x.std(-1,keepdim = True)\n",
        "        # '-1' means last dimension\n",
        "\n",
        "        out = (x-mean)/(std + self.eps)\n",
        "        out = self.gamma * out + self.beta\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-VXHHhqH3dTT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self, d_model, hidden, drop):\n",
        "    super(FFN, self).__init__()\n",
        "\n",
        "    self.lr1 = nn.Linear(d_model, hidden)\n",
        "    self.lr2 = nn.Linear(hidden, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=drop)\n",
        "\n",
        "  def forward(self, x):\n",
        "    temp1 = self.lr1(x)\n",
        "    temp1 = self.relu(x)\n",
        "    temp1 = self.dropout(x)\n",
        "    result = self.lr2(x)\n",
        "    return result"
      ],
      "metadata": {
        "id": "MYmwODcV3dT3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_head, hidden, drop):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(d_model, n_head)\n",
        "    self.norm = LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(drop)\n",
        "    self.ffn = FFN(d_model, hidden, drop)\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    res1_x = x\n",
        "    x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
        "    norm1_x = self.norm(res1_x + x)\n",
        "    x = self.dropout(norm1_x)\n",
        "\n",
        "    res2_x = x\n",
        "    x = self.ffn(x)\n",
        "    norm2_x = self.norm(res2_x + x)\n",
        "    x = self.dropout(norm2_x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "59rieGcW3dUi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, voc_size, d_model, hidden, n_layers, n_head, seq_len, drop, device):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings = voc_size, embed_dim=d_model, padding_idx=1)\n",
        "    self.pe = PE(seq_len, d_model)\n",
        "    self.layers = nn.ModuleList([Encoder(d_model=d_model, n_head=n_head, hidden=hidden, drop=drop) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.embed(x)\n",
        "    pe_x = self.pe(x)\n",
        "    x = x + pe_x\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, src_mask)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "-r7PUpbz3dVG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "8pH2XcCinjo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_head, hidden, drop):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.masked_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "    self.norm = LayerNorm(d_model=d_model)\n",
        "    self.dropout = nn.Dropout(drop)\n",
        "\n",
        "    self.cross_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
        "    self.ffn = FFN(d_model=d_model, hidden=hidden, drop=drop)\n",
        "\n",
        "  def forward(self, x_dec, enc, trg_mask, src_mask):\n",
        "    res1_x = x_dec\n",
        "    x_dec = self.masked_attention(q=x_dec, k=x_dec, v=x_dec, mask=trg_mask)\n",
        "    x = self.norm(res1_x + x_dec)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    if enc is not None:\n",
        "      res2_x = x\n",
        "      x = self.cross_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
        "      x = self.norm(res2_x + x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "    res3_x = x\n",
        "    x = self.ffn(x)\n",
        "    x = self.norm(res3_x + x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "LL_wiy-63dVk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, voc_size, d_model, seq_len, hidden, n_head, n_layer, drop, device):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embed = nn.Embedding(num_embeddings=voc_size, embedding_dim=d_model, padding_idx=1)\n",
        "    self.pe = PE(seq_len=seq_len, d_model=d_model, device='cuda')\n",
        "    self.layers = nn.ModuleList([DecoderLayer(d_model=d_model, n_head=n_head, hidden=hidden, drop=drop) for _ in range(n_layer)])\n",
        "    self.lr = nn.Linear(d_model, voc_size)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x, trg_mask, src_mask):\n",
        "    x = self.embed(x)\n",
        "    pe_x = self.pe(x)\n",
        "    x = x + pe_x\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, trg_mask, src_mask)\n",
        "\n",
        "    x = self.lr(x)\n",
        "    output = self.softmax(x)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "MgLvhtKW3dWG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self,src_pad_idx,trg_pad_idx,trg_sos_idx,enc_voc_size,dec_voc_size,d_model,n_head,max_len, ffn_hidden,n_layers,drop_prob,device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.trg_sos_idx = trg_sos_idx\n",
        "\n",
        "        self.encoder = Encoder(enc_voc_size = enc_voc_size, max_len = max_len, d_model = d_model, ffn_hidden = ffn_hidden,  n_head = n_head,\n",
        "                               n_layers = n_layers, drop_prob = drop_prob, device = device)\n",
        "        self.decoder = Decoder(dec_voc_size = dec_voc_size, max_len = max_len, d_model = d_model, ffn_hidden = ffn_hidden, n_head = n_head,\n",
        "                               n_layers = n_layers, drop_prob = drop_prob, device = device)\n",
        "        self.device = device\n",
        "\n",
        "    def make_pad_mask(self,q,k):\n",
        "        len_q,len_k = q.size(1),k.size(1)\n",
        "        print(len_k)\n",
        "\n",
        "        k = k.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        print(k.shape)\n",
        "\n",
        "        k = k.repeat(1,1,len_q,1)\n",
        "        q = q.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        q = q.repeat(1,1,1,len_k)\n",
        "\n",
        "        mask = k & q\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def make_no_peak_mask(self,q,k):\n",
        "        len_q,len_k = q.size(1),k.size(1)\n",
        "        mask = torch.tril(torch.ones(len_q,len_k)).type(torch.BoolTensor).to(self.device)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def forward(self,src,trg):\n",
        "        src_mask = self.make_pad_mask(src,src)\n",
        "        src_trg_mask = self.make_pad_mask(trg,src)\n",
        "        trg_mask = self.make_pad_mask(trg,trg) * self.make_no_peak_mask(trg,trg)\n",
        "        enc_src = self.encoder(src,src_mask)\n",
        "        output = self.decoder(trg,enc_src,trg_mask,src_trg_mask)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "EwMLW3-W3dWp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4j_Th1IG3dXI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wu7pB00K3dXm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pq6aPV_d3dYL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_F_DQOA3dYq"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}